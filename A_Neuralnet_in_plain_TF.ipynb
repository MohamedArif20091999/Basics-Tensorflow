{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Neuralnet in plain TF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGyLxrfPEh4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "%tensorflow_version 1.x \n",
        "\n",
        "n_inputs=28*28 #MNIST\n",
        "n_hidden1=300\n",
        "n_hidden2=100\n",
        "n_outputs=10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvL7FeOYKnOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ec9c57b-b128-4847-971a-e7f55b497387"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCqIbJruFEJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neuron_layer(X,n_neurons,name,activation=None):\n",
        "  with tf.name_scope(name):\n",
        "    n_inputs=int(X.get_shape()[1])\n",
        "    stddev=2/np.sqrt(n_inputs)\n",
        "    init=tf.truncated_normal((n_inputs,n_neurons),stddev=stddev)\n",
        "    W=tf.Variable(init,name=\"weights\")\n",
        "    b=tf.Variable(tf.zeros([n_neurons]),name=\"biases\")\n",
        "    z=tf.matmul(X,W)+b\n",
        "    if activation==\"relu\":\n",
        "      return tf.nn.relu(z)\n",
        "    else:\n",
        "      return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XniU_h4KIF9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMJGn97RGluB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"dnn\"):\n",
        "  hidden1=neuron_layer(X,n_hidden1,\"hidden1\",activation=\"relu\")\n",
        "  hidden2=neuron_layer(hidden1,n_hidden2,\"hidden2\",activation=\"relu\")\n",
        "  logits=neuron_layer(hidden2,n_outputs,\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJkt9EBtHfWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "  xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
        "  loss=tf.reduce_mean(xentropy,name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4YL84xZIwLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate=0.1\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "  optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  training_op=optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVYhDo56JQsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "  correct=tf.nn.in_top_k(logits,y,1)\n",
        "  accuracy=tf.reduce_mean(tf.cast(correct,tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86IJh9L4JsVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init=tf.global_variables_initializer()\n",
        "saver=tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNWrh_REJ106",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 40\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ia-EjmHJ-04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Hj4ijBKBqw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "ce7f526f-5bbe-460d-a0eb-3fb83bd51e26"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()    \n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Batch accuracy: 1.0 Val accuracy: 0.9566\n",
            "1 Batch accuracy: 0.98 Val accuracy: 0.969\n",
            "2 Batch accuracy: 0.98 Val accuracy: 0.973\n",
            "3 Batch accuracy: 1.0 Val accuracy: 0.9758\n",
            "4 Batch accuracy: 1.0 Val accuracy: 0.978\n",
            "5 Batch accuracy: 1.0 Val accuracy: 0.9798\n",
            "6 Batch accuracy: 1.0 Val accuracy: 0.976\n",
            "7 Batch accuracy: 1.0 Val accuracy: 0.98\n",
            "8 Batch accuracy: 1.0 Val accuracy: 0.9818\n",
            "9 Batch accuracy: 1.0 Val accuracy: 0.9824\n",
            "10 Batch accuracy: 1.0 Val accuracy: 0.983\n",
            "11 Batch accuracy: 1.0 Val accuracy: 0.9834\n",
            "12 Batch accuracy: 1.0 Val accuracy: 0.9828\n",
            "13 Batch accuracy: 1.0 Val accuracy: 0.9834\n",
            "14 Batch accuracy: 1.0 Val accuracy: 0.983\n",
            "15 Batch accuracy: 1.0 Val accuracy: 0.9832\n",
            "16 Batch accuracy: 1.0 Val accuracy: 0.9828\n",
            "17 Batch accuracy: 1.0 Val accuracy: 0.9822\n",
            "18 Batch accuracy: 1.0 Val accuracy: 0.9832\n",
            "19 Batch accuracy: 1.0 Val accuracy: 0.9842\n",
            "20 Batch accuracy: 1.0 Val accuracy: 0.9838\n",
            "21 Batch accuracy: 1.0 Val accuracy: 0.984\n",
            "22 Batch accuracy: 1.0 Val accuracy: 0.9844\n",
            "23 Batch accuracy: 1.0 Val accuracy: 0.9836\n",
            "24 Batch accuracy: 1.0 Val accuracy: 0.9834\n",
            "25 Batch accuracy: 1.0 Val accuracy: 0.9836\n",
            "26 Batch accuracy: 1.0 Val accuracy: 0.9836\n",
            "27 Batch accuracy: 1.0 Val accuracy: 0.9844\n",
            "28 Batch accuracy: 1.0 Val accuracy: 0.984\n",
            "29 Batch accuracy: 1.0 Val accuracy: 0.9844\n",
            "30 Batch accuracy: 1.0 Val accuracy: 0.9838\n",
            "31 Batch accuracy: 1.0 Val accuracy: 0.984\n",
            "32 Batch accuracy: 1.0 Val accuracy: 0.9838\n",
            "33 Batch accuracy: 1.0 Val accuracy: 0.9844\n",
            "34 Batch accuracy: 1.0 Val accuracy: 0.9842\n",
            "35 Batch accuracy: 1.0 Val accuracy: 0.9838\n",
            "36 Batch accuracy: 1.0 Val accuracy: 0.9844\n",
            "37 Batch accuracy: 1.0 Val accuracy: 0.9846\n",
            "38 Batch accuracy: 1.0 Val accuracy: 0.984\n",
            "39 Batch accuracy: 1.0 Val accuracy: 0.9844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_bUMSPWKGLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXw0gnuRLt95",
        "colab_type": "text"
      },
      "source": [
        "Fine bru!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHmSjbbBLwS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}